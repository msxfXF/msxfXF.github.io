---
layout: post
title: 自然语言处理(0)-深度学习用于文本和序列
category: DeepLearning
tags: [DeepLearning]
excerpt_separator: <!-- more -->
excerpt: 介绍使用深度学习模型处理文本数据

typora-copy-images-to: ..\assets\images
---

自然语言处理(0)-深度学习用于文本和序列
# 深度学习用于处理文本和序列

***

## 介绍

介绍使用深度学习模型处理文本数据。

<!-- more -->

深度学习用于处理序列的两种基本的深度学习算法分别是**循环神经网络（recurrent neural network）**和**一维卷积神经网络（1D convnet）**。这些算法的应用有：

- 文档分类和时间序列分类：识别文章主题、书的作者。
- 时间序列对比：评估两个文档或股票行情的相关程度。
- 序列到序列的学习：机器翻译。
- 情感分析：将推文或电影评论的情感划分为正面或负面。
- 时间序列预测：根据天气数据来预测未来天气

***

***

## 处理文本数据

深度学习模型不会接受原始文本作为输入，它只能处理数值张量的文本向量化（vectorize）是指将文本转换为数值张量的过程。

- 将文本分割为单词，并将每个单词转换为一个向量。

- 将文本分割为字符，并将每个字符转换为一个向量。

- 提取单词或字符的n-gram，并将每个n-gram转换为一个向量。n-gram是多个连续单词或字符的集合。

将文本分解而成的单元叫做标记，将文本分解成标记的过程叫做**分词（tokenization）**，主要的方法有：对标记做**one-hot编码（one-hot encoding）**与**词嵌入（word embedding）**。

词袋是一种不保存顺序的分词方法，生成的是集合，而不是序列，往往被用于浅层的语言处理模型（如logistic回归、随机森林）。

## ont-hot 编码

ont-hot 编码是将标记转换为向量最常用最基本的方法。它将每个单词与一个唯一的整数所有相关联，然后将这个整数索引i转换为长度为N的二进制向量（N为词表大小），这个向量只有第i个元素是1，其余元素都为0。

keras内置函数可以实现单词级或字符级的ont-hot 编码。

```python
samples = ["The cat sat on the mat.","The dog ate my homework."]
# 创建分词器
tokenizer = Tokenizer(num_words=1000)
# 构建单词索引
tokenizer.fit_on_texts(samples)
# 转换为ont-hot编码 (列表形式)
sequences = tokenizer.text_to_sequences(samples)
# 或转换为ont-hot编码 (矩阵形式)
matrix = tokenizer.text_to_matrix(samples,mode="binary")
```

ont-hot 编码的一个变体是**ont-hot 散列技巧（ont-hot hashing trick）**，该方法没有为每个单词显式分配索引，保存在字典中，而是将单词散列编码为固定长度的向量，用一个散列/哈希函数来实现。优点在于，避免了维护一个显式的单词索引，节约内存，并允许数据在线编码，缺点是可能会出现**散列/哈希冲突（hash collision）**。

``` python
for i, sample in enumerate(samples):
    for j, word in list(enumerate(sample.split()))[:max_length]:
        index = abs(hash(word)) % dimensionality # 哈希表长度
        results[i,j,index] = 1
```

## 词嵌入

将单词与向量相关联还有另外一种常用的强大方法，就是使用密集的**词向量（word vector）**，也叫做**词嵌入（word embedding）。**one-hot 编码得到的向量是二进制的、稀疏的、维度很高的，而词嵌入是低维的、密集的浮点数向量，是从数据中学习得到的。

获取词嵌入有两种方法：

- 完成主任务的同时学习词嵌入。一开始是随机词向量，然后对这些词向量进行学习。
- 预计算词嵌入，然后加载到模型中。即**预训练词嵌入（pretrained word embedding）**

## 使用Embdding层学习词嵌入

``` python
form keras.layers import Embedding
# Embedding层需要两个参数（标记个数，嵌入维度）
embedding_layer = Embdding(1000,64)
```

可以将Embedding层理解为一个字典，并将整数索引映射为密集向量。它接受整数作为输入，查找后返回相关联的词向量。

它的输入是一个二维整数张量，形状为（samples，sequence_length）。在一批数据中，序列需要有相同的长度，所以较短序列用0填充，较长序列应该被截断。

它的返回是一个三维浮点数张量，形状为（samples，sequence_length，embedding_dimensionality）。然后可以使用RNN层或者一维卷积层来处理这个三维张量（或者使用Flatten层将三维张量展平为二维张量）。

## 使用预训练的词嵌入

常用的词嵌入模型有word2vec、GloVe等等。

之后会详细说明如何使用预训练的词嵌入模型。

