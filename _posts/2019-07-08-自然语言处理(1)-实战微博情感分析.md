---
layout: post
title: è‡ªç„¶è¯­è¨€å¤„ç†(1)-å®æˆ˜å¾®åšæƒ…æ„Ÿåˆ†æ
category: DeepLearning
tags: [DeepLearning]
excerpt_separator: <!-- more -->
excerpt: ä½¿ç”¨word2vecã€jiebaåˆ†è¯ã€BiLSTMæ¨¡å‹å¯¹å¾®åšå†…å®¹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»

typora-copy-images-to: ..\assets\images
---

è‡ªç„¶è¯­è¨€å¤„ç†(1)-å®æˆ˜å¾®åšæƒ…æ„Ÿåˆ†æ
# å®æˆ˜å¾®åšæƒ…æ„Ÿåˆ†æ

***

## ä»‹ç»

ä½¿ç”¨word2vecã€jiebaåˆ†è¯ã€BiLSTMæ¨¡å‹å¯¹å¾®åšå†…å®¹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ã€‚

<!-- more -->

***

***

## æ•´ä½“æ€è·¯

æ¶‰åŠçš„ä¸»è¦å†…å®¹æœ‰ï¼š

- å¾®åšæƒ…æ„Ÿåˆ†ææ•°æ®é›†çš„è·å–ã€‚

- å¯¹jiebaåˆ†è¯è¿›è¡Œç®€è¦ä»‹ç»ï¼Œå¹¶ä½¿ç”¨å®ƒå®Œæˆå¯¹å¾®åšå¥å­çš„åˆ†è¯ä»»åŠ¡ã€‚

- å¯¹word2vecè¯å‘é‡æ¨¡å‹è¿›è¡Œç®€å•ä»‹ç»ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡è¯å‘é‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œè½¬æ¢ã€‚

- ç®€å•ä»‹ç»ï¼Œæ„å»ºå¹¶ä½¿ç”¨BiLSTMæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå®Œæˆæƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚

- ç®—æ³•çš„ä¸è¶³ä¹‹å¤„å’Œéœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚

## å¾®åšæƒ…æ„Ÿåˆ†ææ•°æ®é›†

å¸¸ç”¨çš„æ•°æ®é›†æœ‰ï¼š

- NLP&CC 2013ï¼š
  è¯¥æ•°æ®é›†æ˜¯xmlæ ¼å¼ï¼Œæœ‰å¤§æ¦‚3wæ¡æ•°æ®ï¼Œ17wæ¡æ•°æ®ä½œä¸ºæµ‹è¯•æ ·æœ¬ï¼ˆæ‰‹åŠ¨æ»‘ğŸ”ï¼‰ï¼Œåˆ†ä¸ºnoneã€likeã€disgustã€angerã€happinessã€fearã€sadnessã€surpriseç­‰å‡ ä¸ªç±»åˆ«ï¼Œå…¶ä¸­noneåˆå ç»å¤§å¤šæ•°ã€‚

  å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

  ![1562564298530](/assets/images/1562564298530.png)

- COAE2013ï¼šæ²¡æœ‰è¿›è¡Œä¸‹è½½ï¼Œä¹Ÿå°±ä¸å…·ä½“è¯´æ˜äº†ï¼ŒåŒæ ·æ®è¯´æ•°æ®è¾ƒå°‘ã€‚

- ï¼ˆ~~ä¸çŸ¥é“åå­—çš„æ•°æ®é›†~~ï¼‰:æˆ‘ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªCSDNä¸Šçš„æ•°æ®é›†ï¼Œtxtæ ¼å¼ï¼Œæ•°æ®å·²ç»æ¸…ç†å¥½ï¼Œåˆ†ä¸º6wæ¡ç§¯ææƒ…ç»ªå’Œ6wæ¡æ¶ˆææƒ…ç»ªï¼Œä¸‹è½½åœ°å€åœ¨æ–‡æœ«ã€‚

  å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

  ![1562564354162](/assets/images/1562564354162.png)

## jiebaä»‹ç»

> ç»“å·´ä¸­æ–‡åˆ†è¯æ¶‰åŠåˆ°çš„ç®—æ³•åŒ…æ‹¬ï¼š 
> (1) åŸºäºå‰ç¼€è¯å…¸å®ç°é«˜æ•ˆçš„è¯å›¾æ‰«æï¼Œç”Ÿæˆå¥å­ä¸­æ±‰å­—æ‰€æœ‰å¯èƒ½æˆè¯æƒ…å†µæ‰€æ„æˆçš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAG)ï¼› 
> (2) é‡‡ç”¨äº†åŠ¨æ€è§„åˆ’æŸ¥æ‰¾æœ€å¤§æ¦‚ç‡è·¯å¾„, æ‰¾å‡ºåŸºäºè¯é¢‘çš„æœ€å¤§åˆ‡åˆ†ç»„åˆï¼› 
> (3) å¯¹äºæœªç™»å½•è¯ï¼Œé‡‡ç”¨äº†åŸºäºæ±‰å­—æˆè¯èƒ½åŠ›çš„HMMæ¨¡å‹ï¼Œä½¿ç”¨äº†Viterbiç®—æ³•ã€‚

jiebaçš„ä¸»è¦åŠŸèƒ½æœ‰ï¼š

- åˆ†è¯

- æ·»åŠ è‡ªå®šä¹‰è¯å…¸

- å…³é”®å­—æå–

- è¯æ€§æ ‡æ³¨

æˆ‘ä»¬ä¸»è¦ä½¿ç”¨å®ƒçš„åˆ†è¯åŠŸèƒ½ï¼ŒHMMæ¨¡å‹åŸç†å’Œviterbiç®—æ³•åŸç†åœ¨æ–‡æœ«çš„æœ‰ä¼ é€é—¨ã€‚

jiebaæ”¯æŒä¸‰ç§åˆ†è¯æ¨¡å¼ï¼š

- ç²¾ç¡®æ¨¡å¼ï¼Œé€‚åˆæ–‡æœ¬åˆ†æã€‚
- å…¨æ¨¡å¼ï¼ŒæŠŠæ‰€æœ‰å¯ä»¥æˆè¯çš„è¯è¯­éƒ½æ‰«æå‡ºæ¥ï¼Œä¸èƒ½è§£å†³æ­§ä¹‰ã€‚
- æœç´¢å¼•æ“æ¨¡å¼ï¼Œåœ¨ç²¾ç¡®æ¨¡å¼çš„åŸºç¡€ä¸Šå¯¹é•¿è¯å†æ¬¡åˆ‡åˆ†ï¼Œé€‚åˆæœç´¢å¼•æ“åˆ†è¯ã€‚

åˆ†è¯åŠŸèƒ½è°ƒç”¨æ–¹æ³•

- `jieba.cut` æ¥å—ä¸‰ä¸ªè¾“å…¥å‚æ•°: éœ€è¦åˆ†è¯çš„å­—ç¬¦ä¸²ï¼›cut_all å‚æ•°ç”¨æ¥æ§åˆ¶æ˜¯å¦é‡‡ç”¨å…¨æ¨¡å¼ï¼›HMM å‚æ•°ç”¨æ¥æ§åˆ¶æ˜¯å¦ä½¿ç”¨ HMM æ¨¡å‹

- `jieba.cut_for_search` æ¥å—ä¸¤ä¸ªå‚æ•°ï¼šéœ€è¦åˆ†è¯çš„å­—ç¬¦ä¸²ï¼›æ˜¯å¦ä½¿ç”¨ HMM æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€‚åˆç”¨äºæœç´¢å¼•æ“æ„å»ºå€’æ’ç´¢å¼•çš„åˆ†è¯ï¼Œç²’åº¦æ¯”è¾ƒç»†

- `jieba.cut` ä»¥åŠ `jieba.cut_for_search` è¿”å›çš„ç»“æ„éƒ½æ˜¯ä¸€ä¸ªå¯è¿­ä»£çš„ generatorï¼Œå¯ä»¥ä½¿ç”¨ for å¾ªç¯æ¥è·å¾—åˆ†è¯åå¾—åˆ°çš„æ¯ä¸€ä¸ªè¯è¯­(unicode)ï¼Œæˆ–è€…ç”¨

- `jieba.lcut` ä»¥åŠ `jieba.lcut_for_search` ç›´æ¥è¿”å› list

- `jieba.Tokenizer(dictionary=DEFAULT_DICT)` æ–°å»ºè‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œå¯ç”¨äºåŒæ—¶ä½¿ç”¨ä¸åŒè¯å…¸ã€‚`jieba.dt` ä¸ºé»˜è®¤åˆ†è¯å™¨ï¼Œæ‰€æœ‰å…¨å±€åˆ†è¯ç›¸å…³å‡½æ•°éƒ½æ˜¯è¯¥åˆ†è¯å™¨çš„æ˜ å°„ã€‚

ä»£ç ç¤ºä¾‹ï¼š

``` python
# encoding=utf-8
import jieba

seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # å…¨æ¨¡å¼

seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # ç²¾ç¡®æ¨¡å¼

seg_list = jieba.cut("ä»–æ¥åˆ°äº†ç½‘æ˜“æ­ç ”å¤§å¦")  # é»˜è®¤æ˜¯ç²¾ç¡®æ¨¡å¼
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("å°æ˜ç¡•å£«æ¯•ä¸šäºä¸­å›½ç§‘å­¦é™¢è®¡ç®—æ‰€ï¼Œååœ¨æ—¥æœ¬äº¬éƒ½å¤§å­¦æ·±é€ ")  # æœç´¢å¼•æ“æ¨¡å¼
print(", ".join(seg_list))
```

è¾“å‡º:

``` python
ã€å…¨æ¨¡å¼ã€‘: æˆ‘/ æ¥åˆ°/ åŒ—äº¬/ æ¸…å/ æ¸…åå¤§å­¦/ åå¤§/ å¤§å­¦

ã€ç²¾ç¡®æ¨¡å¼ã€‘: æˆ‘/ æ¥åˆ°/ åŒ—äº¬/ æ¸…åå¤§å­¦

ã€æ–°è¯è¯†åˆ«ã€‘ï¼šä»–, æ¥åˆ°, äº†, ç½‘æ˜“, æ­ç ”, å¤§å¦    (æ­¤å¤„ï¼Œâ€œæ­ç ”â€å¹¶æ²¡æœ‰åœ¨è¯å…¸ä¸­ï¼Œä½†æ˜¯ä¹Ÿè¢«Viterbiç®—æ³•è¯†åˆ«å‡ºæ¥äº†)

ã€æœç´¢å¼•æ“æ¨¡å¼ã€‘ï¼š å°æ˜, ç¡•å£«, æ¯•ä¸š, äº, ä¸­å›½, ç§‘å­¦, å­¦é™¢, ç§‘å­¦é™¢, ä¸­å›½ç§‘å­¦é™¢, è®¡ç®—, è®¡ç®—æ‰€, å, åœ¨, æ—¥æœ¬, äº¬éƒ½, å¤§å­¦, æ—¥æœ¬äº¬éƒ½å¤§å­¦, æ·±é€ 
```

## word2vecä»‹ç»

æˆ‘ä»¬ä½¿ç”¨Chinese Word Vectors ä¸­æ–‡è¯å‘é‡ï¼Œå¯ä»¥ç›´æ¥ä¸‹è½½å¯¹åº”è¯­æ–™åº“ä¸‹é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚

![1562572589185](/assets/images/1562572589185.png)

è¿™é‡Œæˆ‘ä¸‹è½½çš„æ˜¯ç¨ å¯†çŸ©é˜µçš„å¾®åšWordæ–‡ä»¶ã€‚

è¯¥æ¨¡å‹è¯å‘é‡ä¸º300ç»´ï¼Œæµ®ç‚¹æ•°ç²¾ç¡®åˆ°å°æ•°ç‚¹åäº”ä½ï¼Œæœ‰19W+ä¸ªè¯å‘é‡ï¼ŒåŒæ—¶é‡Œé¢åŒ…å«äº†ä¸­æ–‡æ ‡ç‚¹ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![1562573250673](/assets/images/1562573250673.png)

ä¸‹è½½ä¹‹åï¼Œå¯ä»¥æŒ‰ç…§ GitHub **ana_eval_dense.py** é‡Œçš„ä»£ç åŠ è½½è¯å‘é‡æ–‡ä»¶ï¼Œå¹¶è½¬æ¢ä¸ºçŸ©é˜µå½¢å¼

éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

``` python
def read_vectors(path, topn):  # read top n word vectors, i.e. top is 10000
    lines_num, dim = 0, 0
    vectors = {}
    iw = []
    wi = {}
    with open(path, encoding='utf-8', errors='ignore') as f:
        first_line = True
        for line in f:
            if first_line:
                first_line = False
                dim = int(line.rstrip().split()[1])
                continue
            lines_num += 1
            tokens = line.rstrip().split(' ')
            vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])
            iw.append(tokens[0])
            if topn != 0 and lines_num >= topn:
                break
    for i, w in enumerate(iw):
        wi[w] = i
    return vectors, iw, wi, dim

vectors, iw, wi, dim = read_vectors(vectors_path, topn)  # Read top n word vectors. Read all vectors when topn is 0
    # Turn vectors into numpy format and normalize them
    matrix = np.zeros(shape=(len(iw), dim), dtype=np.float32)
    for i, word in enumerate(iw):
        matrix[i, :] = vectors[word]
    matrix = normalize(matrix)
```

**æ³¨æ„ï¼šwindowsä¸‹éœ€è¦å°†with open(path) as f æ”¹æˆ with open(path,encoding="utf-8") as f**



ä¹Ÿå¯ä»¥ä½¿ç”¨è°·æ­Œçš„word2vecçš„pythonåº“ï¼šhttps://pypi.org/project/word2vec/

ç›´æ¥ä½¿ç”¨

``` python
word2vec.load("æ–‡ä»¶ä½ç½®.txt")
```

å³å¯å®Œæˆå¯¼å…¥

## BiLSTMä»‹ç»

ç®€å•æ¥è®²ï¼ŒLSTMæ˜¯åœ¨RNNçš„åŸºç¡€ä¸ŠåŠ å…¥äº†é—å¿˜é—¨å’Œè®°å¿†é—¨ï¼›

BiLSTMæ˜¯åœ¨LSTMçš„åŸºç¡€ä¸Šï¼Œå˜ä¸ºäº†åŒå‘å¾ªç¯ã€‚

ä¸‹å›¾ä¸ºLSTMï¼š

![img](/assets/images/1540354949562.png)

ä¸‹å›¾ä¸ºBi-LSTMï¼š

![img](/assets/images/1540354951193.png)

ä½¿ç”¨Bi-LSTMç”¨äºæƒ…æ„Ÿåˆ†ç±»é—®é¢˜å¯ä»¥å……åˆ†è€ƒè™‘åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯è¯­çš„å…·ä½“æ„Ÿæƒ…è‰²å½©ï¼Œæ›´å¥½çš„æ•æ‰åŒå‘çš„è¯­ä¹‰ä¾èµ–ã€‚

Kerasä¸­å·²ç»é›†æˆäº†Bi-LSTMæ¨¡å‹ï¼Œå»ºç«‹æ¨¡å‹çš„ä»£ç å¦‚ä¸‹:

``` python
# å»ºç«‹çº¿æ€§æ¨¡å‹
model = Sequential()
# æ·»åŠ Bi-LSTMå±‚
model.add(Bidirectional(LSTM(64,dropout=0.2,return_sequences=True),input_shape=(128,300)))
# å°†è¾“å‡ºå¹³å¦åŒ–
model.add(Flatten())
# æ·»åŠ ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆç¥ç»å…ƒï¼‰
model.add(Dense(1))
# ä½¿ç”¨sigmodå‡½æ•°è¿›è¡ŒäºŒåˆ†ç±»
model.add(Activation("sigmoid"))
# æŸå¤±å‡½æ•°ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µï¼Œä¼˜åŒ–å™¨ä½¿ç”¨adamï¼Œè¡¡é‡æ ‡å‡†ä½¿ç”¨å‡†ç¡®ç‡
model.compile(loss="binary_crossentropy", optimizer="adam",metrics=["accuracy"])
```

## å®Œæ•´ä»£ç ï¼ˆç²—ç³™ç‰ˆï¼‰ï¼š

### å¯åœ¨githubä¸­å…‹éš†ï¼šhttps://github.com/msxfXF/NLPWeibo

**train.py**

``` python 
import numpy as np
import random
import word2vec
import jieba
from keras import Sequential
from keras.layers import LSTM,Bidirectional,Activation,Dense,Flatten
from keras_preprocessing import sequence
w2v = word2vec.load("sgns.weibo.word.txt")

strs = []
strs_label = []
with open("data/train_0.txt","r",encoding="utf-8") as f:
	lines = f.readlines()
	for line in lines:
		strs.append(line)
		strs_label.append(0)
with open("data/train_1.txt",encoding="utf-8") as f:
	lines = f.readlines()
	for line in lines:
		strs.append(line)
		strs_label.append(1)
shuffle_index = np.random.permutation(np.arange(len(strs)))
strs = np.array(strs)[shuffle_index]
y = np.array(strs_label)[shuffle_index]

x = np.zeros(shape=(len(strs),128,300), dtype=np.float32)
for i,str in enumerate(strs):
	res_cuts = jieba.cut(str[:128])
	for j,res_cut in enumerate(res_cuts):
		if res_cut in w2v:
			x[i,j,:] = w2v[res_cut]

model = Sequential()
model.add(Bidirectional(LSTM(64,dropout=0.2,return_sequences=True),input_shape=(128,300)))
model.add(Flatten())
model.add(Dense(1))
model.add(Activation("sigmoid"))
model.compile(loss="binary_crossentropy", optimizer="adam",metrics=["accuracy"])
print(model.summary())

model.fit(x,y,validation_split=0.1,batch_size=32, epochs=3)
model.save('weibo.h5')
```

**demo.py**

``` python
import numpy as np
import random
import word2vec
import jieba
import sys
from keras import Sequential,models
from keras.layers import LSTM,Bidirectional,Activation,Dense,Flatten
from keras_preprocessing import sequence


if __name__ == "__main__":
    if(len(sys.argv)>=2):
        
        w2v = word2vec.load(r"sgns.weibo.word.txt")
        x = np.zeros(shape=(1,128,300), dtype=np.float32)
        model = models.load_model('weibo.h5')
        print(model.summary())
        for i,str in enumerate(sys.argv[1:]):
            res_cuts = jieba.cut(str)
            for j,res_cut in enumerate(res_cuts):
                if res_cut in w2v:
                    x[0,j,:] = w2v[res_cut]
                    print(res_cut)
            res = model.predict(x)
            print(str)
            print(res)
    else:
        pass
```

å¯¹æ•°æ®è¿›è¡Œåˆ’åˆ†åï¼Œè®­ç»ƒé›†ï¼šæµ‹è¯•é›†ï¼šéªŒè¯é›† = 107820ï¼š11980ï¼š200ï¼Œè®­ç»ƒå¾—åˆ°çš„ç»“æœåœ¨æµ‹è¯•é›†ä¸Šçš„ç²¾ç¡®åº¦åœ¨96.8%å·¦å³ã€‚ï¼ˆå› ä¸ºå»å­¦è½¦äº†ï¼Œåªè®­ç»ƒäº†3ä¸ªepochï¼Œå…¶å®å·²ç»è¿‡æ‹Ÿåˆäº†ï¼‰

æµ‹è¯•æ•ˆæœï¼š

![1562574977046](/assets/images/1562574977046.png)

## å­˜åœ¨çš„é—®é¢˜

å½“ç„¶è¿˜éå¸¸éå¸¸ä¸å®Œå–„ï¼ï¼ï¼

ä¸ªäººæ„Ÿè§‰æ˜¯æ•°æ®é‡å¤ªå°‘ï¼Œä¸€äº›æ•°æ®çš„ç›¸å…³æ€§æœ‰äº›é«˜ï¼Œæ¯”å¦‚å¾ˆå¤šå¾®åšé‡Œéƒ½æœ‰ â€œå“­äº†â€ è¿™ä¸ªè¯ã€‚

å¦å¤–ï¼Œæ¨¡å‹ä¸€äº›å‚æ•°ä¹Ÿæ²¡æœ‰è¿›è¡Œè°ƒä¼˜ï¼Œä»£ç æ¯”è¾ƒç²—ç³™ï¼Œç›´æ¥è¯»å…¥æ‰€æœ‰æ•°æ®å ç”¨å†…å­˜è¾ƒå¤§ã€‚

![1562575233628](/assets/images/1562575233628.png)

äº‹å®è¯æ˜ï¼Œæƒ³é€šè¿‡12Wå¥è¯ï¼Œ0åŸºç¡€å­¦ä¼šä¸­æ–‡æ‰€æœ‰æƒ…æ„Ÿï¼Œå¯¹æœºå™¨æ¥è®²è¿˜æ˜¯æœ‰ç‚¹éš¾åº¦çš„ï¼

## å‚è€ƒèµ„æ–™å’Œç›¸å…³ç½‘å€

å¾®åšæ•°æ®é›†ï¼š

https://download.csdn.net/download/weixin_38442818/10214750

jiebaåˆ†è¯GitHubï¼š

https://github.com/fxsjy/jieba

ç»“å·´åˆ†è¯HMMæ¨¡å‹åŸç†ï¼š

https://zhuanlan.zhihu.com/p/40502333

HMMéšé©¬å°”ç§‘å¤«æ¨¡å‹ä¼ é€é—¨ï¼š

https://blog.csdn.net/zxm1306192988/article/details/78595933
viterbi ç®—æ³•ä¼ é€é—¨ï¼š

https://www.zhihu.com/question/20136144/answer/37291465

è¯¦ç»†Bi-LSTMä»‹ç»ï¼š

https://www.jiqizhixin.com/articles/2018-10-24-13